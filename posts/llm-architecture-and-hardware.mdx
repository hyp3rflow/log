---
title: "LLM 아키텍처 발전과 하드웨어 메모리의 공진화"
date: 2026/02/21
description: "Transformer 아키텍처의 메모리 병목을 해결해온 소프트웨어·하드웨어 혁신을 사실 기반으로 정리"
tag: llm-study
author: Claude Opus 4.6
---

# LLM 아키텍처 발전과 하드웨어 메모리의 공진화

## 1. Transformer 기본 구조 — 왜 메모리가 병목인가

2017년 Vaswani et al.이 발표한 "Attention Is All You Need"[^1]는 RNN/LSTM을 대체하는 Transformer 구조를 제안했다. 핵심은 **Self-Attention** 메커니즘이다.

<TransformerDecoderDiagram />

### 1.1 Self-Attention 연산

입력 시퀀스 길이를 n, hidden dimension을 d라 하면, Query·Key·Value 행렬을 만들고 다음을 계산한다:

<Math display expr='"Attention"(Q, K, V) = "softmax"((Q K^T) / sqrt(d_k)) dot V' />

이 연산의 시간 복잡도는 O(n² · d)이다. QKᵀ 행렬의 크기는 n × n이므로, 시퀀스 길이가 길어지면 **메모리 사용량이 제곱으로 증가**한다.

### 1.2 KV Cache — 추론 시 메모리 소비의 주범

Autoregressive 추론에서는 매 토큰 생성 시 이전 토큰들의 Key, Value를 재사용한다. 이를 **KV Cache**라 한다.

KV Cache 크기는 다음과 같이 계산된다:

```
KV Cache = 2 × L × n_kv_heads × d_head × seq_len × batch × precision
```

- `2`: Key와 Value 각각
- `L`: Transformer 레이어 수
- `n_kv_heads`: KV Head 수 (MHA에서는 전체 head 수)
- `d_head`: head당 차원 (= hidden_dim / num_heads)
- `seq_len`: 시퀀스 길이
- `precision`: 바이트 수 (FP16 = 2바이트)

예를 들어 LLaMA 3 70B (80 layers, 64 heads, d=8192, GQA 8 KV heads)를 FP16으로 8192 토큰 추론하면:

```
KV Cache = 2 × 80 × 8 × 128 × 8192 × 1 × 2 = 2.68 GB
```

모델 가중치(140 GB, FP16)와 합하면 **142.68 GB**의 VRAM이 필요하다. 배치 크기를 늘리면 KV Cache가 선형으로 증가한다. 이것이 LLM 서빙에서 메모리가 병목이 되는 핵심 이유다.

### 1.3 연산량 분석

Forward pass의 FLOPs는 대략 2 × N × P (N=토큰 수, P=파라미터 수)로 추정된다[^2]. 70B 모델의 8192 토큰 forward pass는 약 1.15 PFLOPs이다. 그러나 실제로는 **연산보다 메모리 대역폭이 먼저 한계에 도달**한다. GPU의 연산 능력(TFLOPs)은 빠르게 증가했지만, 메모리 대역폭은 그에 비해 느리게 성장했기 때문이다. 이를 **Memory Wall** 문제라 한다.

<LLMMemoryCalculator />

---

## 2. 아키텍처 발전 — 메모리 병목을 소프트웨어로 해결하기

### 2.1 Multi-Head → Multi-Query → Grouped-Query Attention

**Multi-Head Attention (MHA)**는 원래 Transformer 논문에서 제안된 방식이다. 모든 head가 독립적인 Q, K, V 프로젝션을 가진다. Head 수가 h이면 KV Cache도 h에 비례한다.

**Multi-Query Attention (MQA)**는 2019년 Shazeer가 제안했다[^3]. 모든 head가 **하나의 K, V를 공유**한다. KV Cache가 h분의 1로 줄어든다. 그러나 품질 저하가 관측되었다.

**Grouped-Query Attention (GQA)**는 2023년 Ainslie et al.이 제안했다[^4]. h개의 Query head를 g개의 그룹으로 나누고, 각 그룹이 하나의 KV head를 공유한다. MHA와 MQA의 중간 지점이다.

```
MHA:  KV heads = h       (예: 64)
GQA:  KV heads = g       (예: 8)   → KV Cache 87.5% 절감
MQA:  KV heads = 1                  → KV Cache 98.4% 절감
```

<AttentionComparisonDiagram />

LLaMA 3는 GQA를 채택하여 8B 모델은 8개, 70B 모델도 8개의 KV head를 사용한다[^5]. 이로 인해 KV Cache 크기가 MHA 대비 87.5% 줄어들면서도 품질 저하는 미미하다.

### 2.2 위치 인코딩 — RoPE와 ALiBi

#### 왜 위치 인코딩이 필요한가

Self-Attention 연산 자체는 **순서에 무관(permutation invariant)**하다. `softmax(QKᵀ)·V`에서 토큰의 순서를 바꿔도 결과가 동일하다. 즉 "나는 밥을 먹는다"와 "밥을 나는 먹는다"를 구분할 수 없다. 이는 RNN/LSTM이 순차적으로 처리하며 자연스럽게 순서를 인식하는 것과 근본적으로 다르다.

따라서 Transformer는 위치 정보를 **외부에서 주입**해야 한다. 어떻게 주입하느냐에 따라 세 가지 방식으로 나뉜다:

#### 1. Sinusoidal Positional Encoding (원래 Transformer)

원래 Transformer[^1]는 위치마다 고정된 sin/cos 벡터를 만들어 임베딩에 **더했다**:

<Math display expr='"PE"("pos", 2i) = sin("pos" / 10000^(2i\/d))' />

이 방식은 **절대 위치(absolute position)**를 인코딩한다. 학습 시 본 시퀀스 길이(예: 512)를 넘어서면 본 적 없는 위치 벡터가 되어 **일반화가 어렵다**. 또한 두 토큰 사이의 상대적 거리를 직접적으로 표현하지 않는다.

#### 2. RoPE (Rotary Position Embedding)

Su et al.(2021)이 제안한 RoPE[^6]는 근본적으로 다른 접근을 취한다. 임베딩에 값을 더하는 대신, **Query와 Key 벡터를 위치에 따라 회전**시킨다:

<Math display expr='f(x_m, m) = x_m dot e^(i m theta)' />

이를 2차원 쌍으로 풀어쓰면, hidden dimension의 연속된 두 원소 (x₂ᵢ, x₂ᵢ₊₁)를 각도 mθᵢ만큼 회전시키는 것이다:

<Math display expr='mat(cos(m theta), -sin(m theta); sin(m theta), cos(m theta)) mat(x_(2i); x_(2i+1))' />

이 방식의 핵심 성질은 **두 토큰의 attention score가 상대 거리에만 의존**한다는 것이다:

<Math display expr='f(q_m)^T dot f(k_n) = g(x_m, x_n, m - n)' />

즉 절대 위치가 아닌 **상대 위치**를 자연스럽게 인코딩한다. Sinusoidal이 "나는 위치 5에 있다"라고 알려준다면, RoPE는 "나는 저 토큰보다 3칸 뒤에 있다"를 인코딩하는 셈이다.

RoPE의 실용적 장점은 **컨텍스트 확장**에 있다. 학습 시 4K 토큰으로 훈련한 모델을 32K나 128K로 확장할 때, **NTK-aware scaling**(θ의 base를 조정)이나 **YaRN**(주파수별 차등 스케일링)으로 외삽이 가능하다. LLaMA, Qwen, Mistral 등 대부분의 최신 모델이 RoPE를 사용한다.

#### 3. ALiBi (Attention with Linear Biases)

Press et al.(2022)이 제안한 ALiBi[^7]는 더 급진적인 접근이다. **위치 임베딩을 아예 사용하지 않는다.** 대신, attention score에 거리에 비례하는 선형 penalty를 직접 더한다:

<Math display expr='"softmax"(q_i dot k_j - m dot |i - j|)' />

여기서 m은 head마다 다른 상수(기하급수적으로 감소)이다. 멀리 있는 토큰일수록 attention score가 낮아지도록 유도하는 것이다. 학습된 파라미터가 없으므로 학습 없이도 더 긴 시퀀스로 외삽이 가능하다. BLOOM (176B)[^8] 모델이 ALiBi를 사용했다.

#### 비교 정리

| | Sinusoidal | RoPE | ALiBi |
|---|---|---|---|
| 위치 유형 | 절대 위치 | 상대 위치 | 상대 거리 bias |
| 적용 위치 | 임베딩에 덧셈 | Q, K에 회전 | attention score에 덧셈 |
| 학습 파라미터 | 없음 | 없음 | 없음 (m은 고정) |
| 외삽 능력 | 약함 | NTK/YaRN으로 강함 | 기본적으로 강함 |
| 채택 모델 | 원래 Transformer | LLaMA, Mistral, Qwen | BLOOM, MPT |

긴 컨텍스트 지원은 메모리 관점에서도 중요하다. 시퀀스 길이가 4K에서 128K로 32배 늘어나면, KV Cache도 32배 증가한다. 위치 인코딩의 발전 — 특히 RoPE의 스케일링 기법 — 이 없었다면 이 정도의 컨텍스트 확장은 불가능했을 것이다.

### 2.3 Mixture of Experts (MoE) — 파라미터는 늘리고 연산량은 유지

Shazeer et al.(2017)이 제안한 MoE 개념[^9]은 최근 Mixtral 8x7B[^10], GPT-4 (추정), DeepSeek-V2 등에서 활발히 사용된다.

MoE의 핵심:
- FFN 레이어를 E개의 Expert로 복제
- 각 토큰은 **Router**가 선택한 top-k개의 Expert만 활성화
- 총 파라미터 수는 크지만, 토큰당 활성 파라미터 수는 작다

Mixtral 8x7B의 경우:
- 총 파라미터: 46.7B (8개 Expert × FFN 파라미터)
- 활성 파라미터: 12.9B (top-2 Expert 선택)
- Dense 13B 모델과 비슷한 연산량으로 더 높은 성능

메모리 관점에서 MoE의 트레이드오프는 명확하다. **모든 Expert의 가중치를 메모리에 올려야 하지만**, 추론 시 연산량(FLOPs)은 줄어든다. 즉 MoE는 연산 효율은 높이지만, VRAM 요구량은 줄이지 않는다 (오히려 늘린다). 이것이 대용량 메모리 하드웨어의 필요성을 더욱 강화한다.

### 2.4 Flash Attention — IO 복잡도의 혁신

Dao et al.(2022)이 제안한 Flash Attention[^11]은 attention 연산의 **메모리 IO**를 최적화한다.

표준 attention은 n × n attention 행렬을 HBM에 materialize한다. Flash Attention은 이를 **타일(tile) 단위로 분할**하여 GPU SRAM(on-chip memory)에서 계산한 뒤, softmax를 online으로 처리한다 (online softmax trick).

| | 표준 Attention | Flash Attention |
|---|---|---|
| HBM 읽기/쓰기 | O(n²) | O(n²d / M) |
| SRAM 사용 | O(n²) | O(M) |
| 속도 (A100, 2K seq) | 1× | 2.4× |
| 속도 (A100, 4K seq) | 1× | 4.3× |

여기서 M은 SRAM 크기이다. A100의 SRAM은 20 MB (192 KB × 108 SM), HBM은 80 GB이다. Flash Attention은 이 **수천 배의 대역폭 차이**를 활용한다.

Flash Attention 2[^12] (2023)는 parallelism을 개선하여 A100에서 이론 최대 대역폭의 72%를 달성했다. Flash Attention 3 (2024)는 H100의 FP8 Tensor Core와 비동기 연산을 활용한다.

---

## 3. 하드웨어 발전 — GPU 메모리의 진화

### 3.1 GPU 메모리 벽: DRAM → GDDR → HBM

GPU 메모리의 역사는 대역폭과의 전쟁이다.

- **GDDR5** (2008): 핀당 5 Gbps, 32-bit 인터페이스. 512-bit 버스의 GPU에서 최대 약 336 GB/s.
- **GDDR6** (2018): 핀당 16 Gbps, 32-bit 인터페이스. 게이밍 GPU에서 약 900 GB/s.

그러나 GDDR은 **핀 수 한계**가 있다. 대역폭을 높이려면 버스 폭을 넓혀야 하는데, PCB 배선이 물리적으로 한계에 도달한다. 이 문제를 해결한 것이 **HBM (High Bandwidth Memory)**이다.

HBM은 DRAM 다이를 수직으로 적층(3D stacking)하고, **TSV (Through-Silicon Via)**로 연결한다. 한 스택의 버스 폭이 1024-bit (HBM1/2) 또는 2048-bit (HBM4)로, GDDR의 32배 이상이다.

<HBMStackDiagram />

### 3.2 HBM 세대별 진화

JEDEC 표준 기준으로 HBM의 세대별 스펙은 다음과 같다[^13]:

| 세대 | 표준 연도 | 핀당 속도 | 스택 구성 | 최대 용량/스택 | 최대 대역폭/스택 |
|---|---|---|---|---|---|
| **HBM1** | 2013 | 1.0 Gb/s | 4-Hi, 8×128bit | 4 GB | 128 GB/s |
| **HBM2** | 2016 | 2.4 Gb/s | 8-Hi, 8×128bit | 8 GB | 307 GB/s |
| **HBM2E** | 2019 | 3.6 Gb/s | 12-Hi, 8×128bit | 24 GB | 461 GB/s |
| **HBM3** | 2022 | 6.4 Gb/s | 12-Hi, 16×64bit | 48 GB | 819 GB/s |
| **HBM3E** | 2023 | 9.8 Gb/s | 16-Hi, 16×64bit | 48 GB | 1,229 GB/s |
| **HBM4** | 2025 | 8.0 Gb/s | 16-Hi, 32×64bit | 64 GB | 2,048 GB/s |

핵심 추세:
1. **적층 수 증가**: 4-Hi → 8-Hi → 12-Hi → 16-Hi. 더 많은 다이를 쌓아 용량 확대.
2. **채널 분할**: HBM3부터 128-bit 채널을 64-bit pseudo-channel로 분할하여 독립 접근성 향상.
3. **핀 속도**: 10년간 1.0 → 9.8 Gb/s로 약 10배 향상.

### 3.3 HBF (High Bandwidth Flash)

삼성이 발표한 **HBF (High Bandwidth Flash)**는 **NAND 플래시 메모리를 HBM과 유사한 방식으로 3D 적층**하여 대역폭을 극대화한 새로운 유형의 메모리이다[^14].

#### HBM은 DRAM, HBF는 NAND Flash

HBM이 DRAM 다이를 TSV로 수직 적층해 대역폭을 높였듯이, HBF는 **V-NAND(3D NAND) 다이를 TSV로 적층**하여 기존 SSD 대비 대역폭을 대폭 향상시킨다. 핵심 차이는 메모리 유형이다:

| | HBM | HBF |
|---|---|---|
| 메모리 유형 | DRAM (휘발성) | NAND Flash (비휘발성) |
| 특성 | 빠른 접근 속도, 비쌈, 작은 용량 | 느리지만, 저렴, 대용량 |
| 역할 | 모델 가중치 + KV Cache (연산 메모리) | 모델 가중치 저장 + 로딩 (스토리지) |
| 대역폭 | ~1,200 GB/s (HBM3E/스택) | 기존 SSD 대비 수 배 향상 |
| 인터페이스 | GPU 다이에 직접 연결 | 프로세서 패키지 근접 배치 |

#### 왜 HBF가 필요한가

LLM의 규모가 커지면서 두 가지 문제가 동시에 발생한다:

1. **HBM 용량 한계**: 모델이 수백 GB에 달하면 HBM만으로는 부족하다. MoE 모델(예: Mixtral 8x7B, ~93 GB FP16)은 특히 그렇다.
2. **SSD는 너무 느리다**: NVMe SSD의 대역폭은 7~14 GB/s 수준으로, HBM의 수백~수천 GB/s에 비해 100배 이상 느리다.

HBF는 이 격차를 메우는 **중간 계층** 역할을 한다. HBM에 올리기엔 너무 크고, SSD에서 읽기엔 너무 느린 데이터 — 예를 들어 MoE에서 현재 활성화되지 않은 Expert의 가중치 — 를 HBF에 배치할 수 있다. GPU가 필요할 때 HBF에서 빠르게 로딩하는 구조다.

#### 메모리 계층의 확장

HBF의 등장으로 AI 추론 시스템의 메모리 계층이 확장된다:

```
GPU SRAM (~26 MB, ~40 TB/s) → 연산 중 임시 데이터
         ↓
HBM (~192 GB, ~8 TB/s) → 활성 가중치 + KV Cache
         ↓
HBF (수 TB급, 수십~수백 GB/s) → 비활성 Expert, 대형 모델 가중치
         ↓
NVMe SSD (수십 TB, ~14 GB/s) → 체크포인트, 데이터셋
```

HBF는 아직 초기 단계이나, LLM의 규모가 계속 커지는 추세에서 HBM과 SSD 사이의 대역폭 격차를 줄이는 핵심 기술이 될 가능성이 있다.

### 3.4 NVIDIA GPU 세대별 대응

데이터센터 GPU의 메모리 스펙 변화는 LLM 발전과 직접 연결된다[^15]:

| GPU | 출시 | 아키텍처 | 메모리 | 용량 | 대역폭 | FP16 성능 |
|---|---|---|---|---|---|---|
| **V100** | 2017 | Volta | HBM2 | 32 GB | 900 GB/s | 125 TFLOPS |
| **A100** | 2020 | Ampere | HBM2E | 80 GB | 2,039 GB/s | 312 TFLOPS |
| **H100** | 2022 | Hopper | HBM3 | 80 GB | 3,350 GB/s | 990 TFLOPS |
| **H200** | 2024 | Hopper | HBM3E | 141 GB | 4,800 GB/s | 990 TFLOPS |
| **B200** | 2024 | Blackwell | HBM3E | 192 GB | 8,000 GB/s | 2,250 TFLOPS |

주목할 점:

- **V100 → A100**: 메모리 32→80 GB (2.5×), 대역폭 900→2,039 GB/s (2.3×). 이 시기에 GPT-3 (175B)가 등장하여 다수의 GPU 병렬화가 필수가 되었다.
- **A100 → H100**: 대역폭 2,039→3,350 GB/s (1.6×), FP16 성능 312→990 TFLOPS (3.2×). Transformer Engine 도입으로 FP8 학습 지원. Flash Attention이 이 하드웨어를 최대로 활용하는 소프트웨어가 되었다.
- **H100 → H200**: 같은 Hopper 아키텍처지만 **메모리만 업그레이드** (80→141 GB, HBM3→HBM3E). 이는 LLM 서빙에서 메모리 용량이 병목임을 NVIDIA도 인식했다는 사실을 보여준다.
- **B200**: 192 GB HBM3E, 8 TB/s 대역폭. 단일 GPU에 70B 모델(FP16 140GB)을 올릴 수 있게 되었다.

### 3.5 메모리 너머의 병목 — 인터커넥트, 기판, 전력

메모리 대역폭만이 병목은 아니다. LLM 학습/추론 시스템의 규모가 커지면서 다른 물리적 한계도 부상하고 있다.

#### GPU 간 인터커넥트: NVLink → 광통신

단일 GPU의 메모리가 아무리 커져도, 수천억 파라미터 모델은 **여러 GPU에 분산**해야 한다. 이때 GPU 간 통신 대역폭이 병목이 된다.

| 세대 | 대역폭 (양방향) | 비고 |
|---|---|---|
| PCIe 4.0 x16 | 64 GB/s | CPU-GPU, GPU-GPU |
| NVLink 3 (A100) | 600 GB/s | GPU 8장 연결 |
| NVLink 4 (H100) | 900 GB/s | NVSwitch로 확장 |
| NVLink 5 (B200) | 1,800 GB/s | 72-GPU NVSwitch 클러스터 |

NVLink는 구리 기반 전기 신호를 사용한다. 그러나 데이터센터 규모에서 **랙 간 통신**은 구리의 물리적 한계(거리에 따른 신호 감쇠, 전력 소비)에 부딪힌다. 이를 해결하기 위해 **광통신(optical interconnect)**이 부상하고 있다.

광통신의 장점:
- **거리 무관한 대역폭**: 구리는 수 미터에서 신호 품질이 급락하지만, 광섬유는 수백 미터~킬로미터에서도 대역폭 유지
- **낮은 전력**: 장거리 전송 시 구리 대비 전력 효율이 수 배 높음
- **대역폭 밀도**: 파장 분할 다중화(WDM)로 단일 광섬유에 수십 채널 전송 가능

NVIDIA의 ConnectX 네트워크 어댑터는 이미 400 Gb/s 광통신을 지원하며, 차세대 1.6 Tb/s 광 인터커넥트가 개발 중이다. 더 나아가 **Co-Packaged Optics(CPO)** — 광학 모듈을 GPU 패키지에 직접 통합하는 기술 — 은 전기-광 변환 지연을 최소화하여 GPU 간 통신을 광속에 가깝게 만들려는 시도다.

#### 유리 기판 (Glass Substrate)

현재 반도체 패키지는 **유기 기판(organic substrate)**을 사용한다. 그러나 유기 기판은 열팽창률이 크고, 미세 배선이 어려우며, 대형 패키지에서 휨(warpage)이 발생한다.

인텔은 2023년에 **유리 기판(glass core substrate)** 기술을 발표했다. 유리는:

- **열팽창 계수(CTE)가 낮아** 대형 패키지에서 치수 안정성이 높음
- **표면 평탄도가 우수**해 더 미세한 배선(2μm 이하 L/S)이 가능
- **유전 손실이 낮아** 고속 신호 전달에 유리

이는 LLM 가속기와 직접 관련된다. HBM 스택을 더 많이 붙이고, 더 큰 다이를 패키징하려면 기판의 물리적 한계를 넘어야 한다. B200의 패키지는 이미 역대 최대 규모인데, 유기 기판의 한계에 근접해 있다. 유리 기판은 차세대 AI 가속기의 패키지 크기를 한 단계 더 키울 수 있는 핵심 기술이다.

#### 전력과 냉각

GPU 전력 소비도 급증하고 있다:

- V100: 300W → A100: 400W → H100: 700W → B200: 1,000W

데이터센터의 랙당 전력 밀도가 50~100 kW에 달하면서, 기존 공랭식 냉각으로는 한계다. **액체 냉각(liquid cooling)**이 필수가 되고 있으며, H100/B200 세대부터는 직접 액냉(direct-to-chip liquid cooling)이 표준으로 자리잡고 있다. 전력 공급 인프라와 냉각 시스템은 이제 AI 클러스터 설계에서 GPU 선택만큼이나 중요한 변수가 되었다.

---

## 4. 공진화 — 아키텍처와 하드웨어의 상호 견인

아키텍처 혁신과 하드웨어 발전은 독립적으로 이루어지지 않았다. 구체적인 사례를 정리한다.

**하드웨어가 아키텍처를 가능하게 한 사례:**

1. **HBM2 (2016) → Transformer (2017)**: HBM2의 등장으로 V100 GPU가 32 GB, 900 GB/s의 스펙을 달성했다. Transformer의 O(n²) attention이 실용적인 규모에서 학습 가능해졌다.

2. **A100 80GB (2020) → GPT-3 175B (2020)**: A100의 80 GB HBM2E 없이는 175B 규모의 모델 학습이 현실적으로 불가능했다. 8-way 텐서 병렬로 모델을 분할해도 GPU당 최소 ~22 GB의 가중치 메모리가 필요했다.

3. **H100의 SRAM 구조 → Flash Attention 3**: H100의 SM당 256 KB 공유 메모리(총 ~26 MB)는 Flash Attention의 타일 크기를 키울 수 있게 했다.

**아키텍처가 하드웨어를 요구한 사례:**

1. **KV Cache 폭증 → HBM 용량 증가**: 32K~128K 컨텍스트의 모델이 등장하면서, KV Cache만으로 수십 GB가 필요해졌다. H200의 141 GB, B200의 192 GB는 이 수요에 직접 대응한다.

2. **MoE의 메모리 요구 → 대용량 VRAM**: Mixtral 8x7B의 46.7B 파라미터를 FP16으로 올리면 ~93 GB. 단일 GPU에 올리려면 H200급 이상이 필요하다.

3. **Flash Attention의 IO 최적화 → HBM 대역폭 경쟁**: Flash Attention이 HBM 대역폭을 bottleneck으로 만들면서, 더 높은 대역폭의 HBM이 직접적인 성능 향상으로 이어지게 되었다.

**소프트웨어 최적화가 하드웨어 요구를 완화한 사례:**

1. **GQA → KV Cache 절감**: MHA에서 GQA로의 전환만으로 KV Cache가 87.5% 줄어들었다. 하드웨어 업그레이드 없이 같은 GPU에서 더 긴 컨텍스트를 처리할 수 있게 되었다.

2. **양자화 (INT8/INT4) → 더 작은 GPU에서 추론 가능**: 4-bit 양자화로 70B 모델을 ~35 GB로 압축하면 A100 40GB에서도 추론이 가능하다.

---

## 5. 결론

정리하면 다음과 같다:

- Transformer의 attention 메커니즘은 O(n²) 메모리를 요구하며, autoregressive 추론의 KV Cache는 시퀀스 길이·배치·레이어 수에 비례하여 증가한다.
- GQA는 KV Cache를 87.5% 절감하고, Flash Attention은 HBM IO를 O(n²)에서 O(n²d / M)으로 줄였다. MoE는 연산량을 줄이되 메모리는 더 요구한다.
- HBM은 10년간 대역폭 128 GB/s → 2,048 GB/s (16×), 용량 4 GB → 64 GB (16×)로 발전했다.
- NVIDIA 데이터센터 GPU는 V100 (32GB, 900 GB/s) → B200 (192GB, 8,000 GB/s)로 용량 6×, 대역폭 8.9× 증가했다.
- 아키텍처 혁신과 하드웨어 발전은 단방향이 아니라 **상호 견인** 관계다. 아키텍처가 하드웨어의 한계를 노출하고, 하드웨어가 새로운 아키텍처를 가능하게 하는 순환이 반복되고 있다.

---

## References

[^1]: Vaswani, A., et al. "Attention Is All You Need." NeurIPS 2017. [arXiv:1706.03762](https://arxiv.org/abs/1706.03762)

[^2]: Kaplan, J., et al. "Scaling Laws for Neural Language Models." OpenAI, 2020. [arXiv:2001.08361](https://arxiv.org/abs/2001.08361)

[^3]: Shazeer, N. "Fast Transformer Decoding: One Write-Head is All You Need." 2019. [arXiv:1911.02150](https://arxiv.org/abs/1911.02150)

[^4]: Ainslie, J., et al. "GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints." EMNLP 2023. [arXiv:2305.13245](https://arxiv.org/abs/2305.13245)

[^5]: Meta. "LLaMA 3 Model Card." 2024. [llama.meta.com](https://llama.meta.com/)

[^6]: Su, J., et al. "RoFormer: Enhanced Transformer with Rotary Position Embedding." 2021. [arXiv:2104.09864](https://arxiv.org/abs/2104.09864)

[^7]: Press, O., Smith, N., Lewis, M. "Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation." ICLR 2022. [arXiv:2108.12409](https://arxiv.org/abs/2108.12409)

[^8]: BigScience Workshop. "BLOOM: A 176B-Parameter Open-Access Multilingual Language Model." 2023. [arXiv:2211.05100](https://arxiv.org/abs/2211.05100)

[^9]: Shazeer, N., et al. "Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer." ICLR 2017. [arXiv:1701.06538](https://arxiv.org/abs/1701.06538)

[^10]: Jiang, A.Q., et al. "Mixtral of Experts." Mistral AI, 2024. [arXiv:2401.04088](https://arxiv.org/abs/2401.04088)

[^11]: Dao, T., et al. "FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness." NeurIPS 2022. [arXiv:2205.14135](https://arxiv.org/abs/2205.14135)

[^12]: Dao, T. "FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning." 2023. [arXiv:2307.08691](https://arxiv.org/abs/2307.08691)

[^13]: JEDEC. High Bandwidth Memory (HBM) DRAM standards: JESD235, JESD235A, JESD235B, JESD235C, JESD235D. [jedec.org](https://www.jedec.org/)

[^14]: Samsung Electronics. "Samsung Develops Industry's First High Bandwidth Flash (HBF) Memory for On-Device AI." Samsung Semiconductor Newsroom.

[^15]: NVIDIA. Data Center GPU product specifications. [nvidia.com/en-us/data-center](https://www.nvidia.com/en-us/data-center/)
