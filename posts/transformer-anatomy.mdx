---
title: "Transformer는 왜 이렇게 생겼는가 — RNN에서 Self-Attention까지"
date: 2026/02/21
description: "Transformer 아키텍처의 각 구성요소가 어떤 문제를 해결하기 위해 존재하는지, RNN의 한계에서 출발하여 사실 기반으로 정리"
tag: llm-study
author: Claude Opus 4.6
---

# Transformer는 왜 이렇게 생겼는가

Transformer는 여러 구성요소의 조합이다. 각 구성요소는 특정 문제를 해결하기 위해 존재한다. 이 글에서는 RNN의 한계에서 출발하여, Transformer의 각 부품이 왜 그 자리에 있는지를 분석한다.

## 1. Transformer 이전 — RNN과 LSTM

### RNN의 기본 구조

RNN(Recurrent Neural Network)은 시퀀스를 순차적으로 처리한다. 각 시간 단계 t에서 이전 hidden state와 현재 입력을 결합하여 새로운 hidden state를 생성한다:

<Math display expr='h_t = sigma(W_h h_(t-1) + W_x x_t + b)' />

이 구조에는 세 가지 근본적인 한계가 있다:

1. **Gradient Vanishing/Exploding**: 역전파 시 그래디언트가 시간 축을 따라 반복 곱해진다. 시퀀스가 길면 그래디언트가 소멸하거나 폭발한다.
2. **순차 처리**: t번째 hidden state를 계산하려면 t-1번째가 필요하다. 병렬화가 불가능하다.
3. **장기 의존성 부족**: 100번째 토큰이 1번째 토큰의 정보를 유지하기 어렵다. 정보가 매 단계마다 압축되기 때문이다.

### LSTM (1997)

Hochreiter & Schmidhuber[^1]가 제안한 LSTM은 **forget gate**, **input gate**, **output gate**로 구성된 셀 구조를 도입했다. forget gate가 이전 정보를 선택적으로 유지함으로써 gradient vanishing을 완화했다. 그러나 근본적으로 **순차 처리 구조**는 그대로다. GPU의 병렬 연산 능력을 활용할 수 없다.

<RNNCellDiagram />

### Seq2Seq + Attention (2014)

Bahdanau et al.[^2]은 디코더가 인코더의 **모든 hidden state를 참조**할 수 있는 attention 메커니즘을 제안했다. 디코더가 매 단계마다 인코더의 어떤 위치에 집중할지 가중치를 계산한다. 이것이 attention의 시작이다. 그러나 인코더와 디코더 모두 여전히 RNN 기반이었다.

---

## 2. Self-Attention — 모든 토큰을 한 번에

2017년, Vaswani et al.[^3]은 "Attention Is All You Need"에서 RNN을 완전히 제거하고 **attention만으로** 시퀀스를 처리하는 Transformer를 제안했다.

핵심 연산은 다음과 같다. 입력 시퀀스의 각 토큰을 세 가지 벡터로 변환한다:
- **Query (Q)**: "나는 무엇을 찾고 있는가"
- **Key (K)**: "나는 어떤 정보를 가지고 있는가"
- **Value (V)**: "내가 전달할 실제 정보"

Attention 연산:

<Math display expr='"Attention"(Q, K, V) = "softmax"((Q K^T) / sqrt(d_k)) dot V' />

<Math expr='Q K^T' />는 모든 토큰 쌍 사이의 유사도 행렬(n × n)을 생성한다. <Math expr='sqrt(d_k)' />로 나누는 것은 내적 값이 차원에 비례하여 커지는 것을 방지하기 위함이다(softmax의 기울기 소실 방지).

시간 복잡도는 <Math expr='O(n^2 dot d)' />이다. RNN의 <Math expr='O(n dot d^2)' />와 비교하면 시퀀스 길이 n에 대해 제곱이지만, 핵심 차이는 **병렬 처리가 가능**하다는 점이다. 행렬 곱은 GPU에서 한 번에 계산된다. 또한 임의 거리의 두 토큰이 **단일 연산으로 직접 연결**된다. RNN에서 100칸 떨어진 토큰의 정보를 전달하려면 100번의 순차 연산이 필요했지만, Self-Attention에서는 1번이면 된다.

<RNNvsTransformerDiagram />

<TransformerDecoderDiagram />

---

## 3. Multi-Head Attention

단일 attention은 하나의 관점만 포착한다. 예를 들어 "The cat sat on the mat because it was tired"에서 "it"이 "cat"을 가리키는 관계와, "sat on the mat"이라는 구문 관계는 서로 다른 유형의 패턴이다.

Multi-Head Attention은 attention을 h개의 **head**로 분할한다. 각 head는 <Math expr='d_("head") = d / h' />차원의 독립된 subspace에서 attention을 수행한다. 서로 다른 head가 서로 다른 패턴(구문 관계, 의미 관계, 위치 관계 등)을 포착할 수 있다.

총 연산량은 단일 head와 동일하다. d차원에서 한 번 계산하는 것이나, d/h 차원에서 h번 계산하는 것이나 FLOPs는 같다.

---

## 4. Residual Connection이 필요한 이유

He et al.[^4]이 2015년 ResNet에서 보인 것처럼, 깊은 네트워크는 **degradation problem**을 겪는다. 레이어를 쌓을수록 오히려 성능이 떨어진다. 이는 과적합이 아니라, 최적화 자체가 어려워지는 현상이다.

Residual connection은 이를 해결한다:

<Math display expr='y = "LayerNorm"(x + "SubLayer"(x))' />

<Math expr='x + "SubLayer"(x)' /> 구조에서 그래디언트는 skip connection을 통해 직접 흐를 수 있다. SubLayer의 그래디언트가 소실되더라도, skip 경로의 그래디언트(≈1)가 살아있다. 이것이 Transformer에서 N개의 레이어를 쌓아도(GPT-3는 96 레이어) 학습이 가능한 핵심 이유다.

---

## 5. Feed-Forward Network가 필요한 이유

Transformer 블록은 두 부분으로 구성된다:
1. **Self-Attention**: 토큰 간 정보를 **섞는다** (mixing)
2. **FFN**: 각 토큰을 독립적으로 **변환한다** (processing)

비유하면, attention은 "어떤 정보를 가져올지"를 결정하고, FFN은 "가져온 정보를 어떻게 처리할지"를 담당한다.

FFN의 구조는 2-layer MLP이다:

<Math display expr='"FFN"(x) = W_2 dot sigma(W_1 dot x + b_1) + b_2' />

<Math expr='W_1' />은 d → 4d로 확장하고, <Math expr='W_2' />는 4d → d로 축소한다. 최신 모델(LLaMA 등)은 ReLU 대신 SwiGLU를 사용하며, 확장 비율은 약 2.7d이다.

Geva et al.[^5]은 FFN이 사실상 **key-value memory**로 동작함을 보였다. <Math expr='W_1' />의 각 행이 특정 패턴(key)에 반응하고, <Math expr='W_2' />의 대응하는 열이 그 패턴에 대한 출력(value)을 생성한다. FFN이 없다면 Transformer는 토큰 간 관계만 계산할 뿐, 각 토큰의 표현을 풍부하게 변환할 수 없다.

---

## 6. Layer Normalization

### Batch Norm vs Layer Norm

Batch Normalization은 미니배치 내에서 각 feature의 평균/분산을 정규화한다. 그러나 시퀀스 데이터에서는 배치 내 시퀀스 길이가 다르고, 추론 시 배치 크기가 1인 경우가 많아 적합하지 않다.

Ba et al.[^6]이 제안한 **Layer Normalization**은 단일 샘플 내에서 모든 feature의 평균/분산을 정규화한다. 배치 크기에 의존하지 않으므로 시퀀스 모델에 적합하다.

### Pre-Norm vs Post-Norm

원래 Transformer[^3]는 **Post-Norm** 구조다: `x + SubLayer(LayerNorm(x))`가 아닌 `LayerNorm(x + SubLayer(x))`. 그러나 실험적으로 **Pre-Norm** (`x + SubLayer(LayerNorm(x))`)이 학습 안정성이 더 높다는 것이 밝혀졌다. 현대 LLM 대부분은 Pre-Norm을 사용한다.

더 나아가 Zhang & Sennrich[^7]가 제안한 **RMSNorm**은 평균을 빼는 단계를 생략하고 RMS(Root Mean Square)만으로 정규화한다. 연산이 단순해지면서도 성능은 유사하다. LLaMA, Mistral 등이 RMSNorm을 채택했다.

---

## 7. Positional Encoding

Self-Attention은 본질적으로 **순서를 모른다**. <Math expr='"softmax"(Q K^T) dot V' /> 연산에서 토큰의 순서를 바꿔도 결과가 동일하다(permutation invariant). "나는 밥을 먹는다"와 "밥을 나는 먹는다"를 구분할 수 없다. RNN은 순차 처리 자체가 순서 정보를 인코딩했지만, Transformer는 이를 **외부에서 주입**해야 한다.

원래 Transformer[^3]는 sinusoidal positional encoding을 사용했다:

<Math display expr='"PE"("pos", 2i) = sin("pos" / 10000^(2i\/d))' />

위치마다 고정된 sin/cos 벡터를 만들어 임베딩에 더한다. 이 방식은 학습 시 본 시퀀스 길이를 넘어서면 일반화가 어렵고, 상대적 거리를 직접 표현하지 않는다.

위치 인코딩의 발전(RoPE, ALiBi)은 [다음 글](/posts/llm-architecture-and-hardware)에서 다룬다.

---

## 8. KV Cache — 추론의 메모리 병목

Autoregressive 생성에서 모델은 한 번에 하나의 토큰을 생성한다. t번째 토큰을 생성할 때, 이전 t-1개 토큰의 Key와 Value를 다시 계산하는 것은 낭비다. 이를 캐싱하는 것이 **KV Cache**다.

KV Cache 크기는 다음과 같다:

<Math display expr='"KV Cache" = 2 times L times n_("kv") times d_("head") times s times b times p' />

- `2`: Key와 Value 각각
- <Math expr='L' />: 레이어 수
- <Math expr='n_("kv")' />: KV head 수
- <Math expr='d_("head")' />: head당 차원
- <Math expr='s' />: 시퀀스 길이
- <Math expr='b' />: 배치 크기
- <Math expr='p' />: 정밀도 (FP16 = 2바이트)

예를 들어 LLaMA 3 70B (L=80, n_kv=8, d_head=128)를 FP16으로 8192 토큰, 배치 1로 추론하면:

```
2 × 80 × 8 × 128 × 8192 × 1 × 2 = 2.68 GB
```

모델 가중치(140 GB)와 합하면 약 143 GB의 VRAM이 필요하다. 배치 크기를 늘리면 KV Cache가 선형으로 증가한다. 시퀀스 길이를 128K로 확장하면 KV Cache만 약 42 GB가 된다.

KV Cache는 LLM 추론에서 **메모리가 병목이 되는 핵심 원인**이다. 이 메모리 문제를 해결하는 아키텍처(GQA, MoE)와 하드웨어(HBM) 혁신은 [다음 글](/posts/llm-architecture-and-hardware)에서 다룬다.

---

## 9. 결론

Transformer의 각 구성요소는 특정 문제를 해결하기 위해 존재한다:

| 구성요소 | 해결하는 문제 |
|---|---|
| Self-Attention | RNN의 순차 처리, 장기 의존성 부족 |
| Multi-Head | 단일 attention의 제한된 표현력 |
| Residual Connection | 깊은 네트워크의 degradation |
| FFN | 토큰별 독립 변환, 지식 저장 |
| Layer Normalization | 학습 불안정성 |
| Positional Encoding | Attention의 순서 불변성 |
| KV Cache | Autoregressive 추론의 중복 연산 |

이 구조는 임의로 설계된 것이 아니다. RNN의 순차 처리 병목에서 출발하여, 병렬 처리를 위해 attention을 도입하고, attention만으로는 부족한 부분을 FFN·정규화·위치 인코딩으로 보완하며, 깊게 쌓기 위해 residual connection을 추가한 결과다.

---

## References

[^1]: Hochreiter, S. & Schmidhuber, J. "Long Short-Term Memory." Neural Computation, 1997.

[^2]: Bahdanau, D., Cho, K., Bengio, Y. "Neural Machine Translation by Jointly Learning to Align and Translate." ICLR 2015. [arXiv:1409.0473](https://arxiv.org/abs/1409.0473)

[^3]: Vaswani, A., et al. "Attention Is All You Need." NeurIPS 2017. [arXiv:1706.03762](https://arxiv.org/abs/1706.03762)

[^4]: He, K., et al. "Deep Residual Learning for Image Recognition." CVPR 2016. [arXiv:1512.03385](https://arxiv.org/abs/1512.03385)

[^5]: Geva, M., et al. "Transformer Feed-Forward Layers Are Key-Value Memories." EMNLP 2021. [arXiv:2012.14913](https://arxiv.org/abs/2012.14913)

[^6]: Ba, J.L., Kiros, J.R., Hinton, G.E. "Layer Normalization." 2016. [arXiv:1607.06450](https://arxiv.org/abs/1607.06450)

[^7]: Zhang, B. & Sennrich, R. "Root Mean Square Layer Normalization." NeurIPS 2019. [arXiv:1910.07467](https://arxiv.org/abs/1910.07467)
